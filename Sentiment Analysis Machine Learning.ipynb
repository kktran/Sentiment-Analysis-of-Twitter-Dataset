{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x1ef2cfc50>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dropping @username, changing sentiment to boolean, dropping empty tweets\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv',encoding = \"ISO-8859-1\", header = None)\n",
    "pattern = re.compile('@[A-Za-z0-9_]*')\n",
    "df[6] = df[5].replace(pattern,\"\")\n",
    "df[0] = df[0] == 4\n",
    "df.drop([1,2,3,4],axis = 1, inplace = True)\n",
    "df.drop(df[df[6].str.isspace()].index, inplace = True)\n",
    "df.columns = ['sentiment','original','final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>original</th>\n",
       "      <th>final</th>\n",
       "      <th>no_punc</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>http://twitpic.com/2y1zl - Awww, that's a bum...</td>\n",
       "      <td>httptwitpiccom2y1zl  awww thats a bummer  you...</td>\n",
       "      <td>[httptwitpiccom2y1zl, awww, thats, a, bummer, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "      <td>[is, upset, that, he, cant, update, his, faceb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>I dived many times for the ball. Managed to s...</td>\n",
       "      <td>i dived many times for the ball managed to sa...</td>\n",
       "      <td>[i, dived, many, times, for, the, ball, manage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am...</td>\n",
       "      <td>no its not behaving at all im mad why am i he...</td>\n",
       "      <td>[no, its, not, behaving, at, all, im, mad, why...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>not the whole crew</td>\n",
       "      <td>not the whole crew</td>\n",
       "      <td>[not, the, whole, crew]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>Need a hug</td>\n",
       "      <td>Need a hug</td>\n",
       "      <td>need a hug</td>\n",
       "      <td>[need, a, hug]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "      <td>hey  long time no see! Yes.. Rains a bit ,onl...</td>\n",
       "      <td>hey  long time no see yes rains a bit only a ...</td>\n",
       "      <td>[hey, long, time, no, see, yes, rains, a, bit,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>False</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "      <td>nope they didn't have it</td>\n",
       "      <td>nope they didnt have it</td>\n",
       "      <td>[nope, they, didnt, have, it]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>False</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "      <td>que me muera ?</td>\n",
       "      <td>que me muera</td>\n",
       "      <td>[que, me, muera]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                           original  \\\n",
       "0      False  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1      False  is upset that he can't update his Facebook by ...   \n",
       "2      False  @Kenichan I dived many times for the ball. Man...   \n",
       "3      False    my whole body feels itchy and like its on fire    \n",
       "4      False  @nationwideclass no, it's not behaving at all....   \n",
       "5      False                      @Kwesidei not the whole crew    \n",
       "6      False                                        Need a hug    \n",
       "7      False  @LOLTrish hey  long time no see! Yes.. Rains a...   \n",
       "8      False               @Tatiana_K nope they didn't have it    \n",
       "9      False                          @twittera que me muera ?    \n",
       "\n",
       "                                               final  \\\n",
       "0   http://twitpic.com/2y1zl - Awww, that's a bum...   \n",
       "1  is upset that he can't update his Facebook by ...   \n",
       "2   I dived many times for the ball. Managed to s...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4   no, it's not behaving at all. i'm mad. why am...   \n",
       "5                                not the whole crew    \n",
       "6                                        Need a hug    \n",
       "7   hey  long time no see! Yes.. Rains a bit ,onl...   \n",
       "8                          nope they didn't have it    \n",
       "9                                    que me muera ?    \n",
       "\n",
       "                                             no_punc  \\\n",
       "0   httptwitpiccom2y1zl  awww thats a bummer  you...   \n",
       "1  is upset that he cant update his facebook by t...   \n",
       "2   i dived many times for the ball managed to sa...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4   no its not behaving at all im mad why am i he...   \n",
       "5                                not the whole crew    \n",
       "6                                        need a hug    \n",
       "7   hey  long time no see yes rains a bit only a ...   \n",
       "8                           nope they didnt have it    \n",
       "9                                     que me muera     \n",
       "\n",
       "                                               split  \n",
       "0  [httptwitpiccom2y1zl, awww, thats, a, bummer, ...  \n",
       "1  [is, upset, that, he, cant, update, his, faceb...  \n",
       "2  [i, dived, many, times, for, the, ball, manage...  \n",
       "3  [my, whole, body, feels, itchy, and, like, its...  \n",
       "4  [no, its, not, behaving, at, all, im, mad, why...  \n",
       "5                            [not, the, whole, crew]  \n",
       "6                                     [need, a, hug]  \n",
       "7  [hey, long, time, no, see, yes, rains, a, bit,...  \n",
       "8                      [nope, they, didnt, have, it]  \n",
       "9                                   [que, me, muera]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove punctuation and capital letters\n",
    "import string\n",
    "df['no_punc'] = df['final'].str.replace('[{}]'.format(string.punctuation), '').str.lower()\n",
    "df['split'] = df['no_punc'].str.split()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Positive and Negative Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a+', 'abound', 'abounds', 'abundance', 'abundant']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# positive and negative word lists\n",
    "with open(\"positive-words.txt\") as f:\n",
    "    content = f.readlines()\n",
    "content\n",
    "positive_words = [line.strip() for line in content if line[0] != ';' and not line.isspace()]\n",
    "positive_words[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2-faced', '2-faces', 'abnormal', 'abolish', 'abominable']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"negative-words.txt\") as f:\n",
    "    content = f.readlines()\n",
    "content\n",
    "negative_words = [line.strip() for line in content if line[0] != ';' and not line.isspace()]\n",
    "negative_words[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def categorize_string(tweet):\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    count = Counter(tweet.split())\n",
    "    for key,val in count.items():\n",
    "        if key in positive_words:\n",
    "            positive_count += 1\n",
    "        if key in negative_words:\n",
    "            negative_count += 1\n",
    "    if positive_count > negative_count:\n",
    "        return 1\n",
    "    if negative_count > positive_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>original</th>\n",
       "      <th>final</th>\n",
       "      <th>no_punc</th>\n",
       "      <th>split</th>\n",
       "      <th>count_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>http://twitpic.com/2y1zl - Awww, that's a bum...</td>\n",
       "      <td>httptwitpiccom2y1zl  awww thats a bummer  you...</td>\n",
       "      <td>[httptwitpiccom2y1zl, awww, thats, a, bummer, ...</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "      <td>[is, upset, that, he, cant, update, his, faceb...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>I dived many times for the ball. Managed to s...</td>\n",
       "      <td>i dived many times for the ball managed to sa...</td>\n",
       "      <td>[i, dived, many, times, for, the, ball, manage...</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am...</td>\n",
       "      <td>no its not behaving at all im mad why am i he...</td>\n",
       "      <td>[no, its, not, behaving, at, all, im, mad, why...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>not the whole crew</td>\n",
       "      <td>not the whole crew</td>\n",
       "      <td>[not, the, whole, crew]</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>Need a hug</td>\n",
       "      <td>Need a hug</td>\n",
       "      <td>need a hug</td>\n",
       "      <td>[need, a, hug]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "      <td>hey  long time no see! Yes.. Rains a bit ,onl...</td>\n",
       "      <td>hey  long time no see yes rains a bit only a ...</td>\n",
       "      <td>[hey, long, time, no, see, yes, rains, a, bit,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>False</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "      <td>nope they didn't have it</td>\n",
       "      <td>nope they didnt have it</td>\n",
       "      <td>[nope, they, didnt, have, it]</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>False</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "      <td>que me muera ?</td>\n",
       "      <td>que me muera</td>\n",
       "      <td>[que, me, muera]</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>False</td>\n",
       "      <td>spring break in plain city... it's snowing</td>\n",
       "      <td>spring break in plain city... it's snowing</td>\n",
       "      <td>spring break in plain city its snowing</td>\n",
       "      <td>[spring, break, in, plain, city, its, snowing]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>False</td>\n",
       "      <td>I just re-pierced my ears</td>\n",
       "      <td>I just re-pierced my ears</td>\n",
       "      <td>i just repierced my ears</td>\n",
       "      <td>[i, just, repierced, my, ears]</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>False</td>\n",
       "      <td>@caregiving I couldn't bear to watch it.  And ...</td>\n",
       "      <td>I couldn't bear to watch it.  And I thought t...</td>\n",
       "      <td>i couldnt bear to watch it  and i thought the...</td>\n",
       "      <td>[i, couldnt, bear, to, watch, it, and, i, thou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>False</td>\n",
       "      <td>@octolinz16 It it counts, idk why I did either...</td>\n",
       "      <td>It it counts, idk why I did either. you never...</td>\n",
       "      <td>it it counts idk why i did either you never t...</td>\n",
       "      <td>[it, it, counts, idk, why, i, did, either, you...</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>False</td>\n",
       "      <td>@smarrison i would've been the first, but i di...</td>\n",
       "      <td>i would've been the first, but i didn't have ...</td>\n",
       "      <td>i wouldve been the first but i didnt have a g...</td>\n",
       "      <td>[i, wouldve, been, the, first, but, i, didnt, ...</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>False</td>\n",
       "      <td>@iamjazzyfizzle I wish I got to watch it with ...</td>\n",
       "      <td>I wish I got to watch it with you!! I miss yo...</td>\n",
       "      <td>i wish i got to watch it with you i miss you ...</td>\n",
       "      <td>[i, wish, i, got, to, watch, it, with, you, i,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>False</td>\n",
       "      <td>Hollis' death scene will hurt me severely to w...</td>\n",
       "      <td>Hollis' death scene will hurt me severely to w...</td>\n",
       "      <td>hollis death scene will hurt me severely to wa...</td>\n",
       "      <td>[hollis, death, scene, will, hurt, me, severel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>False</td>\n",
       "      <td>about to file taxes</td>\n",
       "      <td>about to file taxes</td>\n",
       "      <td>about to file taxes</td>\n",
       "      <td>[about, to, file, taxes]</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>False</td>\n",
       "      <td>@LettyA ahh ive always wanted to see rent  lov...</td>\n",
       "      <td>ahh ive always wanted to see rent  love the s...</td>\n",
       "      <td>ahh ive always wanted to see rent  love the s...</td>\n",
       "      <td>[ahh, ive, always, wanted, to, see, rent, love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>False</td>\n",
       "      <td>@FakerPattyPattz Oh dear. Were you drinking ou...</td>\n",
       "      <td>Oh dear. Were you drinking out of the forgott...</td>\n",
       "      <td>oh dear were you drinking out of the forgotte...</td>\n",
       "      <td>[oh, dear, were, you, drinking, out, of, the, ...</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>False</td>\n",
       "      <td>@alydesigns i was out most of the day so didn'...</td>\n",
       "      <td>i was out most of the day so didn't get much ...</td>\n",
       "      <td>i was out most of the day so didnt get much d...</td>\n",
       "      <td>[i, was, out, most, of, the, day, so, didnt, g...</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>False</td>\n",
       "      <td>one of my friend called me, and asked to meet ...</td>\n",
       "      <td>one of my friend called me, and asked to meet ...</td>\n",
       "      <td>one of my friend called me and asked to meet w...</td>\n",
       "      <td>[one, of, my, friend, called, me, and, asked, ...</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>False</td>\n",
       "      <td>@angry_barista I baked you a cake but I ated it</td>\n",
       "      <td>I baked you a cake but I ated it</td>\n",
       "      <td>i baked you a cake but i ated it</td>\n",
       "      <td>[i, baked, you, a, cake, but, i, ated, it]</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>False</td>\n",
       "      <td>this week is not going as i had hoped</td>\n",
       "      <td>this week is not going as i had hoped</td>\n",
       "      <td>this week is not going as i had hoped</td>\n",
       "      <td>[this, week, is, not, going, as, i, had, hoped]</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>False</td>\n",
       "      <td>blagh class at 8 tomorrow</td>\n",
       "      <td>blagh class at 8 tomorrow</td>\n",
       "      <td>blagh class at 8 tomorrow</td>\n",
       "      <td>[blagh, class, at, 8, tomorrow]</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>False</td>\n",
       "      <td>I hate when I have to call and wake people up</td>\n",
       "      <td>I hate when I have to call and wake people up</td>\n",
       "      <td>i hate when i have to call and wake people up</td>\n",
       "      <td>[i, hate, when, i, have, to, call, and, wake, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>False</td>\n",
       "      <td>Just going to cry myself to sleep after watchi...</td>\n",
       "      <td>Just going to cry myself to sleep after watchi...</td>\n",
       "      <td>just going to cry myself to sleep after watchi...</td>\n",
       "      <td>[just, going, to, cry, myself, to, sleep, afte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>False</td>\n",
       "      <td>im sad now  Miss.Lilly</td>\n",
       "      <td>im sad now  Miss.Lilly</td>\n",
       "      <td>im sad now  misslilly</td>\n",
       "      <td>[im, sad, now, misslilly]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>False</td>\n",
       "      <td>ooooh.... LOL  that leslie.... and ok I won't ...</td>\n",
       "      <td>ooooh.... LOL  that leslie.... and ok I won't ...</td>\n",
       "      <td>ooooh lol  that leslie and ok i wont do it aga...</td>\n",
       "      <td>[ooooh, lol, that, leslie, and, ok, i, wont, d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>False</td>\n",
       "      <td>Meh... Almost Lover is the exception... this t...</td>\n",
       "      <td>Meh... Almost Lover is the exception... this t...</td>\n",
       "      <td>meh almost lover is the exception this track g...</td>\n",
       "      <td>[meh, almost, lover, is, the, exception, this,...</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599969</th>\n",
       "      <td>True</td>\n",
       "      <td>@davepell you're the undisputed authority on t...</td>\n",
       "      <td>you're the undisputed authority on the topic....</td>\n",
       "      <td>youre the undisputed authority on the topic  ...</td>\n",
       "      <td>[youre, the, undisputed, authority, on, the, t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599970</th>\n",
       "      <td>True</td>\n",
       "      <td>Thanks @eastwestchic &amp;amp; @wangyip Thanks! Th...</td>\n",
       "      <td>Thanks  &amp;amp;  Thanks! That was just what I wa...</td>\n",
       "      <td>thanks  amp  thanks that was just what i was l...</td>\n",
       "      <td>[thanks, amp, thanks, that, was, just, what, i...</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599971</th>\n",
       "      <td>True</td>\n",
       "      <td>@marttn thanks Martin. not the most imaginativ...</td>\n",
       "      <td>thanks Martin. not the most imaginative inter...</td>\n",
       "      <td>thanks martin not the most imaginative interf...</td>\n",
       "      <td>[thanks, martin, not, the, most, imaginative, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599972</th>\n",
       "      <td>True</td>\n",
       "      <td>@MikeJonesPhoto Congrats Mike  Way to go!</td>\n",
       "      <td>Congrats Mike  Way to go!</td>\n",
       "      <td>congrats mike  way to go</td>\n",
       "      <td>[congrats, mike, way, to, go]</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599973</th>\n",
       "      <td>True</td>\n",
       "      <td>http://twitpic.com/7jp4n - OMG! Office Space.....</td>\n",
       "      <td>http://twitpic.com/7jp4n - OMG! Office Space.....</td>\n",
       "      <td>httptwitpiccom7jp4n  omg office space i wanna ...</td>\n",
       "      <td>[httptwitpiccom7jp4n, omg, office, space, i, w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599974</th>\n",
       "      <td>True</td>\n",
       "      <td>@yrclndstnlvr ahaha nooo you were just away fr...</td>\n",
       "      <td>ahaha nooo you were just away from everyone e...</td>\n",
       "      <td>ahaha nooo you were just away from everyone e...</td>\n",
       "      <td>[ahaha, nooo, you, were, just, away, from, eve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599975</th>\n",
       "      <td>True</td>\n",
       "      <td>@BizCoachDeb  Hey, I'm baack! And, thanks so m...</td>\n",
       "      <td>Hey, I'm baack! And, thanks so much for all ...</td>\n",
       "      <td>hey im baack and thanks so much for all thos...</td>\n",
       "      <td>[hey, im, baack, and, thanks, so, much, for, a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599976</th>\n",
       "      <td>True</td>\n",
       "      <td>@mattycus Yeah, my conscience would be clear i...</td>\n",
       "      <td>Yeah, my conscience would be clear in that ca...</td>\n",
       "      <td>yeah my conscience would be clear in that case</td>\n",
       "      <td>[yeah, my, conscience, would, be, clear, in, t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599977</th>\n",
       "      <td>True</td>\n",
       "      <td>@MayorDorisWolfe Thats my girl - dishing out t...</td>\n",
       "      <td>Thats my girl - dishing out the &amp;quot;advice&amp;...</td>\n",
       "      <td>thats my girl  dishing out the quotadvicequot</td>\n",
       "      <td>[thats, my, girl, dishing, out, the, quotadvic...</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599978</th>\n",
       "      <td>True</td>\n",
       "      <td>@shebbs123 i second that</td>\n",
       "      <td>i second that</td>\n",
       "      <td>i second that</td>\n",
       "      <td>[i, second, that]</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599979</th>\n",
       "      <td>True</td>\n",
       "      <td>In the garden</td>\n",
       "      <td>In the garden</td>\n",
       "      <td>in the garden</td>\n",
       "      <td>[in, the, garden]</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599980</th>\n",
       "      <td>True</td>\n",
       "      <td>@myheartandmind jo jen by nemuselo zrovna tÃ© ...</td>\n",
       "      <td>jo jen by nemuselo zrovna tÃ© holce ael co nic</td>\n",
       "      <td>jo jen by nemuselo zrovna tã© holce ael co nic</td>\n",
       "      <td>[jo, jen, by, nemuselo, zrovna, tã©, holce, ae...</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599981</th>\n",
       "      <td>True</td>\n",
       "      <td>Another Commenting Contest! [;: Yay!!!  http:/...</td>\n",
       "      <td>Another Commenting Contest! [;: Yay!!!  http:/...</td>\n",
       "      <td>another commenting contest  yay  httptinyurlco...</td>\n",
       "      <td>[another, commenting, contest, yay, httptinyur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599982</th>\n",
       "      <td>True</td>\n",
       "      <td>@thrillmesoon i figured out how to see my twee...</td>\n",
       "      <td>i figured out how to see my tweets and facebo...</td>\n",
       "      <td>i figured out how to see my tweets and facebo...</td>\n",
       "      <td>[i, figured, out, how, to, see, my, tweets, an...</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599983</th>\n",
       "      <td>True</td>\n",
       "      <td>@oxhot theri tomorrow, drinking coffee, talkin...</td>\n",
       "      <td>theri tomorrow, drinking coffee, talking abou...</td>\n",
       "      <td>theri tomorrow drinking coffee talking about ...</td>\n",
       "      <td>[theri, tomorrow, drinking, coffee, talking, a...</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599984</th>\n",
       "      <td>True</td>\n",
       "      <td>You heard it here first -- We're having a girl...</td>\n",
       "      <td>You heard it here first -- We're having a girl...</td>\n",
       "      <td>you heard it here first  were having a girl ho...</td>\n",
       "      <td>[you, heard, it, here, first, were, having, a,...</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599985</th>\n",
       "      <td>True</td>\n",
       "      <td>if ur the lead singer in a band, beware fallin...</td>\n",
       "      <td>if ur the lead singer in a band, beware fallin...</td>\n",
       "      <td>if ur the lead singer in a band beware falling...</td>\n",
       "      <td>[if, ur, the, lead, singer, in, a, band, bewar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599986</th>\n",
       "      <td>True</td>\n",
       "      <td>@tarayqueen too much ads on my blog.</td>\n",
       "      <td>too much ads on my blog.</td>\n",
       "      <td>too much ads on my blog</td>\n",
       "      <td>[too, much, ads, on, my, blog]</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599987</th>\n",
       "      <td>True</td>\n",
       "      <td>@La_r_a NEVEER  I think that you both will get...</td>\n",
       "      <td>NEVEER  I think that you both will get on wel...</td>\n",
       "      <td>neveer  i think that you both will get on wel...</td>\n",
       "      <td>[neveer, i, think, that, you, both, will, get,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599988</th>\n",
       "      <td>True</td>\n",
       "      <td>@Roy_Everitt ha- good job. that's right - we g...</td>\n",
       "      <td>ha- good job. that's right - we gotta throw t...</td>\n",
       "      <td>ha good job thats right  we gotta throw that ...</td>\n",
       "      <td>[ha, good, job, thats, right, we, gotta, throw...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599989</th>\n",
       "      <td>True</td>\n",
       "      <td>@Ms_Hip_Hop im glad ur doing well</td>\n",
       "      <td>im glad ur doing well</td>\n",
       "      <td>im glad ur doing well</td>\n",
       "      <td>[im, glad, ur, doing, well]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599990</th>\n",
       "      <td>True</td>\n",
       "      <td>WOOOOO! Xbox is back</td>\n",
       "      <td>WOOOOO! Xbox is back</td>\n",
       "      <td>wooooo xbox is back</td>\n",
       "      <td>[wooooo, xbox, is, back]</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599991</th>\n",
       "      <td>True</td>\n",
       "      <td>@rmedina @LaTati Mmmm  That sounds absolutely ...</td>\n",
       "      <td>Mmmm  That sounds absolutely perfect... but ...</td>\n",
       "      <td>mmmm  that sounds absolutely perfect but my ...</td>\n",
       "      <td>[mmmm, that, sounds, absolutely, perfect, but,...</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599992</th>\n",
       "      <td>True</td>\n",
       "      <td>ReCoVeRiNg FrOm ThE lOnG wEeKeNd</td>\n",
       "      <td>ReCoVeRiNg FrOm ThE lOnG wEeKeNd</td>\n",
       "      <td>recovering from the long weekend</td>\n",
       "      <td>[recovering, from, the, long, weekend]</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>True</td>\n",
       "      <td>@Cliff_Forster Yeah, that does work better tha...</td>\n",
       "      <td>Yeah, that does work better than just waiting...</td>\n",
       "      <td>yeah that does work better than just waiting ...</td>\n",
       "      <td>[yeah, that, does, work, better, than, just, w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>True</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "      <td>just woke up having no school is the best feel...</td>\n",
       "      <td>[just, woke, up, having, no, school, is, the, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>True</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "      <td>thewdbcom  very cool to hear old walt intervie...</td>\n",
       "      <td>[thewdbcom, very, cool, to, hear, old, walt, i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>True</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "      <td>are you ready for your mojo makeover ask me fo...</td>\n",
       "      <td>[are, you, ready, for, your, mojo, makeover, a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>True</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "      <td>happy 38th birthday to my boo of alll time tup...</td>\n",
       "      <td>[happy, 38th, birthday, to, my, boo, of, alll,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>True</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "      <td>happy #charitytuesday</td>\n",
       "      <td>happy charitytuesday</td>\n",
       "      <td>[happy, charitytuesday]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1597281 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                           original  \\\n",
       "0            False  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1            False  is upset that he can't update his Facebook by ...   \n",
       "2            False  @Kenichan I dived many times for the ball. Man...   \n",
       "3            False    my whole body feels itchy and like its on fire    \n",
       "4            False  @nationwideclass no, it's not behaving at all....   \n",
       "5            False                      @Kwesidei not the whole crew    \n",
       "6            False                                        Need a hug    \n",
       "7            False  @LOLTrish hey  long time no see! Yes.. Rains a...   \n",
       "8            False               @Tatiana_K nope they didn't have it    \n",
       "9            False                          @twittera que me muera ?    \n",
       "10           False        spring break in plain city... it's snowing    \n",
       "11           False                         I just re-pierced my ears    \n",
       "12           False  @caregiving I couldn't bear to watch it.  And ...   \n",
       "13           False  @octolinz16 It it counts, idk why I did either...   \n",
       "14           False  @smarrison i would've been the first, but i di...   \n",
       "15           False  @iamjazzyfizzle I wish I got to watch it with ...   \n",
       "16           False  Hollis' death scene will hurt me severely to w...   \n",
       "17           False                               about to file taxes    \n",
       "18           False  @LettyA ahh ive always wanted to see rent  lov...   \n",
       "19           False  @FakerPattyPattz Oh dear. Were you drinking ou...   \n",
       "20           False  @alydesigns i was out most of the day so didn'...   \n",
       "21           False  one of my friend called me, and asked to meet ...   \n",
       "22           False   @angry_barista I baked you a cake but I ated it    \n",
       "23           False             this week is not going as i had hoped    \n",
       "24           False                         blagh class at 8 tomorrow    \n",
       "25           False     I hate when I have to call and wake people up    \n",
       "26           False  Just going to cry myself to sleep after watchi...   \n",
       "27           False                             im sad now  Miss.Lilly   \n",
       "28           False  ooooh.... LOL  that leslie.... and ok I won't ...   \n",
       "29           False  Meh... Almost Lover is the exception... this t...   \n",
       "...            ...                                                ...   \n",
       "1599969       True  @davepell you're the undisputed authority on t...   \n",
       "1599970       True  Thanks @eastwestchic &amp; @wangyip Thanks! Th...   \n",
       "1599971       True  @marttn thanks Martin. not the most imaginativ...   \n",
       "1599972       True          @MikeJonesPhoto Congrats Mike  Way to go!   \n",
       "1599973       True  http://twitpic.com/7jp4n - OMG! Office Space.....   \n",
       "1599974       True  @yrclndstnlvr ahaha nooo you were just away fr...   \n",
       "1599975       True  @BizCoachDeb  Hey, I'm baack! And, thanks so m...   \n",
       "1599976       True  @mattycus Yeah, my conscience would be clear i...   \n",
       "1599977       True  @MayorDorisWolfe Thats my girl - dishing out t...   \n",
       "1599978       True                          @shebbs123 i second that    \n",
       "1599979       True                                     In the garden    \n",
       "1599980       True  @myheartandmind jo jen by nemuselo zrovna tÃ© ...   \n",
       "1599981       True  Another Commenting Contest! [;: Yay!!!  http:/...   \n",
       "1599982       True  @thrillmesoon i figured out how to see my twee...   \n",
       "1599983       True  @oxhot theri tomorrow, drinking coffee, talkin...   \n",
       "1599984       True  You heard it here first -- We're having a girl...   \n",
       "1599985       True  if ur the lead singer in a band, beware fallin...   \n",
       "1599986       True              @tarayqueen too much ads on my blog.    \n",
       "1599987       True  @La_r_a NEVEER  I think that you both will get...   \n",
       "1599988       True  @Roy_Everitt ha- good job. that's right - we g...   \n",
       "1599989       True                 @Ms_Hip_Hop im glad ur doing well    \n",
       "1599990       True                              WOOOOO! Xbox is back    \n",
       "1599991       True  @rmedina @LaTati Mmmm  That sounds absolutely ...   \n",
       "1599992       True                  ReCoVeRiNg FrOm ThE lOnG wEeKeNd    \n",
       "1599994       True  @Cliff_Forster Yeah, that does work better tha...   \n",
       "1599995       True  Just woke up. Having no school is the best fee...   \n",
       "1599996       True  TheWDB.com - Very cool to hear old Walt interv...   \n",
       "1599997       True  Are you ready for your MoJo Makeover? Ask me f...   \n",
       "1599998       True  Happy 38th Birthday to my boo of alll time!!! ...   \n",
       "1599999       True  happy #charitytuesday @theNSPCC @SparksCharity...   \n",
       "\n",
       "                                                     final  \\\n",
       "0         http://twitpic.com/2y1zl - Awww, that's a bum...   \n",
       "1        is upset that he can't update his Facebook by ...   \n",
       "2         I dived many times for the ball. Managed to s...   \n",
       "3          my whole body feels itchy and like its on fire    \n",
       "4         no, it's not behaving at all. i'm mad. why am...   \n",
       "5                                      not the whole crew    \n",
       "6                                              Need a hug    \n",
       "7         hey  long time no see! Yes.. Rains a bit ,onl...   \n",
       "8                                nope they didn't have it    \n",
       "9                                          que me muera ?    \n",
       "10             spring break in plain city... it's snowing    \n",
       "11                              I just re-pierced my ears    \n",
       "12        I couldn't bear to watch it.  And I thought t...   \n",
       "13        It it counts, idk why I did either. you never...   \n",
       "14        i would've been the first, but i didn't have ...   \n",
       "15        I wish I got to watch it with you!! I miss yo...   \n",
       "16       Hollis' death scene will hurt me severely to w...   \n",
       "17                                    about to file taxes    \n",
       "18        ahh ive always wanted to see rent  love the s...   \n",
       "19        Oh dear. Were you drinking out of the forgott...   \n",
       "20        i was out most of the day so didn't get much ...   \n",
       "21       one of my friend called me, and asked to meet ...   \n",
       "22                       I baked you a cake but I ated it    \n",
       "23                  this week is not going as i had hoped    \n",
       "24                              blagh class at 8 tomorrow    \n",
       "25          I hate when I have to call and wake people up    \n",
       "26       Just going to cry myself to sleep after watchi...   \n",
       "27                                  im sad now  Miss.Lilly   \n",
       "28       ooooh.... LOL  that leslie.... and ok I won't ...   \n",
       "29       Meh... Almost Lover is the exception... this t...   \n",
       "...                                                    ...   \n",
       "1599969   you're the undisputed authority on the topic....   \n",
       "1599970  Thanks  &amp;  Thanks! That was just what I wa...   \n",
       "1599971   thanks Martin. not the most imaginative inter...   \n",
       "1599972                          Congrats Mike  Way to go!   \n",
       "1599973  http://twitpic.com/7jp4n - OMG! Office Space.....   \n",
       "1599974   ahaha nooo you were just away from everyone e...   \n",
       "1599975    Hey, I'm baack! And, thanks so much for all ...   \n",
       "1599976   Yeah, my conscience would be clear in that ca...   \n",
       "1599977   Thats my girl - dishing out the &quot;advice&...   \n",
       "1599978                                     i second that    \n",
       "1599979                                     In the garden    \n",
       "1599980    jo jen by nemuselo zrovna tÃ© holce ael co nic    \n",
       "1599981  Another Commenting Contest! [;: Yay!!!  http:/...   \n",
       "1599982   i figured out how to see my tweets and facebo...   \n",
       "1599983   theri tomorrow, drinking coffee, talking abou...   \n",
       "1599984  You heard it here first -- We're having a girl...   \n",
       "1599985  if ur the lead singer in a band, beware fallin...   \n",
       "1599986                          too much ads on my blog.    \n",
       "1599987   NEVEER  I think that you both will get on wel...   \n",
       "1599988   ha- good job. that's right - we gotta throw t...   \n",
       "1599989                             im glad ur doing well    \n",
       "1599990                              WOOOOO! Xbox is back    \n",
       "1599991    Mmmm  That sounds absolutely perfect... but ...   \n",
       "1599992                  ReCoVeRiNg FrOm ThE lOnG wEeKeNd    \n",
       "1599994   Yeah, that does work better than just waiting...   \n",
       "1599995  Just woke up. Having no school is the best fee...   \n",
       "1599996  TheWDB.com - Very cool to hear old Walt interv...   \n",
       "1599997  Are you ready for your MoJo Makeover? Ask me f...   \n",
       "1599998  Happy 38th Birthday to my boo of alll time!!! ...   \n",
       "1599999                          happy #charitytuesday       \n",
       "\n",
       "                                                   no_punc  \\\n",
       "0         httptwitpiccom2y1zl  awww thats a bummer  you...   \n",
       "1        is upset that he cant update his facebook by t...   \n",
       "2         i dived many times for the ball managed to sa...   \n",
       "3          my whole body feels itchy and like its on fire    \n",
       "4         no its not behaving at all im mad why am i he...   \n",
       "5                                      not the whole crew    \n",
       "6                                              need a hug    \n",
       "7         hey  long time no see yes rains a bit only a ...   \n",
       "8                                 nope they didnt have it    \n",
       "9                                           que me muera     \n",
       "10                 spring break in plain city its snowing    \n",
       "11                               i just repierced my ears    \n",
       "12        i couldnt bear to watch it  and i thought the...   \n",
       "13        it it counts idk why i did either you never t...   \n",
       "14        i wouldve been the first but i didnt have a g...   \n",
       "15        i wish i got to watch it with you i miss you ...   \n",
       "16       hollis death scene will hurt me severely to wa...   \n",
       "17                                    about to file taxes    \n",
       "18        ahh ive always wanted to see rent  love the s...   \n",
       "19        oh dear were you drinking out of the forgotte...   \n",
       "20        i was out most of the day so didnt get much d...   \n",
       "21       one of my friend called me and asked to meet w...   \n",
       "22                       i baked you a cake but i ated it    \n",
       "23                  this week is not going as i had hoped    \n",
       "24                              blagh class at 8 tomorrow    \n",
       "25          i hate when i have to call and wake people up    \n",
       "26       just going to cry myself to sleep after watchi...   \n",
       "27                                   im sad now  misslilly   \n",
       "28       ooooh lol  that leslie and ok i wont do it aga...   \n",
       "29       meh almost lover is the exception this track g...   \n",
       "...                                                    ...   \n",
       "1599969   youre the undisputed authority on the topic  ...   \n",
       "1599970  thanks  amp  thanks that was just what i was l...   \n",
       "1599971   thanks martin not the most imaginative interf...   \n",
       "1599972                           congrats mike  way to go   \n",
       "1599973  httptwitpiccom7jp4n  omg office space i wanna ...   \n",
       "1599974   ahaha nooo you were just away from everyone e...   \n",
       "1599975    hey im baack and thanks so much for all thos...   \n",
       "1599976    yeah my conscience would be clear in that case    \n",
       "1599977     thats my girl  dishing out the quotadvicequot    \n",
       "1599978                                     i second that    \n",
       "1599979                                     in the garden    \n",
       "1599980    jo jen by nemuselo zrovna tã© holce ael co nic    \n",
       "1599981  another commenting contest  yay  httptinyurlco...   \n",
       "1599982   i figured out how to see my tweets and facebo...   \n",
       "1599983   theri tomorrow drinking coffee talking about ...   \n",
       "1599984  you heard it here first  were having a girl ho...   \n",
       "1599985  if ur the lead singer in a band beware falling...   \n",
       "1599986                           too much ads on my blog    \n",
       "1599987   neveer  i think that you both will get on wel...   \n",
       "1599988   ha good job thats right  we gotta throw that ...   \n",
       "1599989                             im glad ur doing well    \n",
       "1599990                               wooooo xbox is back    \n",
       "1599991    mmmm  that sounds absolutely perfect but my ...   \n",
       "1599992                  recovering from the long weekend    \n",
       "1599994   yeah that does work better than just waiting ...   \n",
       "1599995  just woke up having no school is the best feel...   \n",
       "1599996  thewdbcom  very cool to hear old walt intervie...   \n",
       "1599997  are you ready for your mojo makeover ask me fo...   \n",
       "1599998  happy 38th birthday to my boo of alll time tup...   \n",
       "1599999                           happy charitytuesday       \n",
       "\n",
       "                                                     split  count_metric  \n",
       "0        [httptwitpiccom2y1zl, awww, thats, a, bummer, ...            -5  \n",
       "1        [is, upset, that, he, cant, update, his, faceb...             0  \n",
       "2        [i, dived, many, times, for, the, ball, manage...            -5  \n",
       "3        [my, whole, body, feels, itchy, and, like, its...            -5  \n",
       "4        [no, its, not, behaving, at, all, im, mad, why...             0  \n",
       "5                                  [not, the, whole, crew]            -5  \n",
       "6                                           [need, a, hug]             1  \n",
       "7        [hey, long, time, no, see, yes, rains, a, bit,...             1  \n",
       "8                            [nope, they, didnt, have, it]            -5  \n",
       "9                                         [que, me, muera]            -5  \n",
       "10          [spring, break, in, plain, city, its, snowing]             0  \n",
       "11                          [i, just, repierced, my, ears]            -5  \n",
       "12       [i, couldnt, bear, to, watch, it, and, i, thou...             0  \n",
       "13       [it, it, counts, idk, why, i, did, either, you...            -5  \n",
       "14       [i, wouldve, been, the, first, but, i, didnt, ...            -5  \n",
       "15       [i, wish, i, got, to, watch, it, with, you, i,...             0  \n",
       "16       [hollis, death, scene, will, hurt, me, severel...             0  \n",
       "17                                [about, to, file, taxes]            -5  \n",
       "18       [ahh, ive, always, wanted, to, see, rent, love...             1  \n",
       "19       [oh, dear, were, you, drinking, out, of, the, ...            -5  \n",
       "20       [i, was, out, most, of, the, day, so, didnt, g...            -5  \n",
       "21       [one, of, my, friend, called, me, and, asked, ...            -5  \n",
       "22              [i, baked, you, a, cake, but, i, ated, it]            -5  \n",
       "23         [this, week, is, not, going, as, i, had, hoped]            -5  \n",
       "24                         [blagh, class, at, 8, tomorrow]            -5  \n",
       "25       [i, hate, when, i, have, to, call, and, wake, ...             0  \n",
       "26       [just, going, to, cry, myself, to, sleep, afte...             0  \n",
       "27                               [im, sad, now, misslilly]             0  \n",
       "28       [ooooh, lol, that, leslie, and, ok, i, wont, d...             0  \n",
       "29       [meh, almost, lover, is, the, exception, this,...            -5  \n",
       "...                                                    ...           ...  \n",
       "1599969  [youre, the, undisputed, authority, on, the, t...             1  \n",
       "1599970  [thanks, amp, thanks, that, was, just, what, i...            -5  \n",
       "1599971  [thanks, martin, not, the, most, imaginative, ...             1  \n",
       "1599972                      [congrats, mike, way, to, go]            -5  \n",
       "1599973  [httptwitpiccom7jp4n, omg, office, space, i, w...             0  \n",
       "1599974  [ahaha, nooo, you, were, just, away, from, eve...             0  \n",
       "1599975  [hey, im, baack, and, thanks, so, much, for, a...             1  \n",
       "1599976  [yeah, my, conscience, would, be, clear, in, t...             1  \n",
       "1599977  [thats, my, girl, dishing, out, the, quotadvic...            -5  \n",
       "1599978                                  [i, second, that]            -5  \n",
       "1599979                                  [in, the, garden]            -5  \n",
       "1599980  [jo, jen, by, nemuselo, zrovna, tã©, holce, ae...            -5  \n",
       "1599981  [another, commenting, contest, yay, httptinyur...             1  \n",
       "1599982  [i, figured, out, how, to, see, my, tweets, an...            -5  \n",
       "1599983  [theri, tomorrow, drinking, coffee, talking, a...            -5  \n",
       "1599984  [you, heard, it, here, first, were, having, a,...            -5  \n",
       "1599985  [if, ur, the, lead, singer, in, a, band, bewar...             0  \n",
       "1599986                     [too, much, ads, on, my, blog]            -5  \n",
       "1599987  [neveer, i, think, that, you, both, will, get,...             1  \n",
       "1599988  [ha, good, job, thats, right, we, gotta, throw...             1  \n",
       "1599989                        [im, glad, ur, doing, well]             1  \n",
       "1599990                           [wooooo, xbox, is, back]            -5  \n",
       "1599991  [mmmm, that, sounds, absolutely, perfect, but,...            -5  \n",
       "1599992             [recovering, from, the, long, weekend]            -5  \n",
       "1599994  [yeah, that, does, work, better, than, just, w...             1  \n",
       "1599995  [just, woke, up, having, no, school, is, the, ...             1  \n",
       "1599996  [thewdbcom, very, cool, to, hear, old, walt, i...             1  \n",
       "1599997  [are, you, ready, for, your, mojo, makeover, a...             1  \n",
       "1599998  [happy, 38th, birthday, to, my, boo, of, alll,...             1  \n",
       "1599999                            [happy, charitytuesday]             1  \n",
       "\n",
       "[1597281 rows x 6 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"count_metric\"] = df['no_punc'].apply(categorize_string)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[df['count_metric']== 0]['count_metric'] = -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[df['count_metric']==-1]['metric'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4115612719364971"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['count_metric']==df['sentiment']])/len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['no_punc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df['sentiment']\n",
    "X = X.tocsc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_traincv, X_test, y_traincv, y_test = train_test_split(X,y, test_size = .2, random_state = 3)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_traincv, y_traincv, test_size = .25, random_state = 3)\n",
    "multi = MultinomialNB()\n",
    "model = multi.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.780167596891\n",
      "0.823243263548\n"
     ]
    }
   ],
   "source": [
    "print(model.score(X_test,y_test))\n",
    "print(model.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search for min_df parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-06 0.780483071221\n",
      "5e-06 0.78084618852\n",
      "1e-05 0.780254557748\n"
     ]
    }
   ],
   "source": [
    "min_dfs = [.000001,0.000005, .00001]\n",
    "for mdf in min_dfs:\n",
    "    vectorizer1 = CountVectorizer(min_df = mdf)\n",
    "    X = vectorizer1.fit_transform(df['no_punc'])\n",
    "    y = df['sentiment']\n",
    "    X_traincv, X_test, y_traincv, y_test = train_test_split(X,y, test_size = .2, random_state = 3)\n",
    "    X_train, X_cv, y_train, y_cv = train_test_split(X_traincv, y_traincv, test_size = .25, random_state = 3)\n",
    "    modeldf = multi.fit(X_train,y_train)\n",
    "    print(mdf, modeldf.score(X_cv,y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_min_df = .000005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search for alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A:\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "A:\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.779506410899\n",
      "0.5 0.780210733247\n",
      "1 0.78084618852\n",
      "5 0.781528598618\n",
      "10 0.78112791746\n"
     ]
    }
   ],
   "source": [
    "vectorizer1 = CountVectorizer(min_df = best_min_df)\n",
    "X = vectorizer1.fit_transform(df['no_punc'])\n",
    "y = df['sentiment']\n",
    "X_traincv, X_test, y_traincv, y_test = train_test_split(X,y, test_size = .2, random_state = 3)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_traincv, y_traincv, test_size = .25, random_state = 3)\n",
    "\n",
    "model2 = MultinomialNB()\n",
    "alphas = [.1,.5,1,5,10]\n",
    "for a in alphas:\n",
    "    model2 = MultinomialNB(alpha = a)\n",
    "    model2.fit(X_train,y_train)\n",
    "    print(a,accuracy_score(model2.predict(X_cv),y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78186109554650551"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = MultinomialNB(alpha = 5.0)\n",
    "model3.fit(X_train,y_train)\n",
    "accuracy_score(model3.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using English Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.781528598618 0.767113467895\n"
     ]
    }
   ],
   "source": [
    "vectorizer_stopwords = CountVectorizer(min_df = best_min_df, stop_words = 'english')\n",
    "Xsw = vectorizer_stopwords.fit_transform(df['no_punc'])\n",
    "ysw = df['sentiment']\n",
    "Xsw_traincv, Xsw_test, ysw_traincv, ysw_test = train_test_split(Xsw,ysw, test_size = .2, random_state = 3)\n",
    "Xsw_train, Xsw_cv, ysw_train, ysw_cv = train_test_split(Xsw_traincv, ysw_traincv, test_size = .25, random_state = 3)\n",
    "model3 = MultinomialNB(alpha = 5.0).fit(X_train,y_train)\n",
    "model_stopwords = MultinomialNB(alpha = 5.0).fit(Xsw_train,ysw_train)\n",
    "print(accuracy_score(model3.predict(X_cv),y_cv),accuracy_score(model_stopwords.predict(Xsw_cv),ysw_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-06 0.766061679856\n",
      "5e-06 0.766262020435\n",
      "1e-05 0.765895772814\n"
     ]
    }
   ],
   "source": [
    "min_dfs = [.000001,0.000005, .00001]\n",
    "for mdf in min_dfs:\n",
    "    vectorizer1 = CountVectorizer(min_df = mdf, stop_words = 'english')\n",
    "    Xsw = vectorizer1.fit_transform(df['no_punc'])\n",
    "    ysw = df['sentiment']\n",
    "    Xsw_traincv, Xsw_test, ysw_traincv, ysw_test = train_test_split(Xsw,ysw, test_size = .2, random_state = 3)\n",
    "    Xsw_train, Xsw_cv, ysw_train, ysw_cv = train_test_split(Xsw_traincv, ysw_traincv, test_size = .25, random_state = 3)\n",
    "    modeldf = MultinomialNB().fit(Xsw_train,ysw_train)\n",
    "    print(mdf, modeldf.score(Xsw_cv,ysw_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.764950415707\n",
      "0.5 0.765920815386\n",
      "1 0.766262020435\n",
      "5 0.767113467895\n",
      "10 0.767060252429\n"
     ]
    }
   ],
   "source": [
    "alphas = [.1,.5,1,5,10]\n",
    "for a in alphas:\n",
    "    model_stopwords = MultinomialNB(alpha = a)\n",
    "    model_stopwords.fit(Xsw_train,ysw_train)\n",
    "    print(a,accuracy_score(model_stopwords.predict(Xsw_cv),ysw_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using stop words result in a worse sentiment classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfvectorizer = TfidfVectorizer(min_df=1)\n",
    "Xtfidf=tfidfvectorizer.fit_transform(df['no_punc'])\n",
    "ytfidf = df['sentiment']\n",
    "Xtfidf_traincv, Xtfidf_test, ytfidf_traincv, ytfidf_test = train_test_split(Xtfidf,ytfidf, test_size = .2, random_state = 3)\n",
    "Xtfidf_train, Xtfidf_cv, ytfidf_train, ytfidf_cv = train_test_split(Xtfidf_traincv, ytfidf_traincv, test_size = .25, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77247016030326465"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tfidf = MultinomialNB().fit(Xtfidf_train,ytfidf_train)\n",
    "accuracy_score(model_tfidf.predict(Xtfidf_test),ytfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5e-06 0.773756010217\n",
      "1e-05 0.774103475909\n",
      "5e-05 0.772594660924\n",
      "0.0001 0.77076655314\n"
     ]
    }
   ],
   "source": [
    "min_dfs = [.000005,.00001,.00005,.0001]\n",
    "for mdf in min_dfs:\n",
    "    tfidfvectorizer1 = TfidfVectorizer(min_df = mdf)\n",
    "    Xtfidf = tfidfvectorizer1.fit_transform(df['no_punc'])\n",
    "    ytfidf = df['sentiment']\n",
    "    Xtfidf_traincv, Xtfidf_test, ytfidf_traincv, ytfidf_test = train_test_split(Xtfidf,ytfidf, test_size = .2, random_state = 3)\n",
    "    Xtfidf_train, Xtfidf_cv, ytfidf_train, ytfidf_cv = train_test_split(Xtfidf_traincv, ytfidf_traincv, test_size = .25, random_state = 3)\n",
    "    modeldf = MultinomialNB().fit(Xtfidf_train,ytfidf_train)\n",
    "    print(mdf, accuracy_score(modeldf.predict(Xtfidf_cv),ytfidf_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_min_df_tfidf = .00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.755900656115\n",
      "1.0 0.755960132225\n",
      "5.0 0.756467244315\n",
      "10.0 0.756914880297\n",
      "50.0 0.758289091455\n",
      "100.0 0.75783206451\n",
      "500.0 0.751490033056\n"
     ]
    }
   ],
   "source": [
    "vectorizer1 = TfidfVectorizer(min_df = best_min_df_tfidf)\n",
    "X = vectorizer1.fit_transform(df['no_punc'])\n",
    "y = df['sentiment']\n",
    "Xtfidf_traincv, Xtfidf_test, ytfidf_traincv, ytfidf_test = train_test_split(Xtfidf,ytfidf, test_size = .2, random_state = 3)\n",
    "Xtfidf_train, Xtfidf_cv, ytfidf_train, ytfidf_cv = train_test_split(Xtfidf_traincv, ytfidf_traincv, test_size = .25, random_state = 3)\n",
    "\n",
    "alphas = [.5,1.0,5.0,10.0,50.0,100.0,500.]\n",
    "for a in alphas:\n",
    "    model2 = MultinomialNB(alpha = a)\n",
    "    model2.fit(Xtfidf_train,ytfidf_train)\n",
    "    print(a,accuracy_score(model2.predict(Xtfidf_cv),ytfidf_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_alpha_tfidf = 50.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77340612351584093"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer1 = TfidfVectorizer(min_df = best_min_df_tfidf)\n",
    "model3 = MultinomialNB(alpha = best_alpha_tfidf)\n",
    "model3.fit(Xtfidf_train,ytfidf_train)\n",
    "accuracy_score(model3.predict(Xtfidf_test),ytfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA and NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 1000\n",
    "tf_vectorizer = CountVectorizer(max_features = n_features)\n",
    "X_lda = tf_vectorizer.fit_transform(df['no_punc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_topics = 100\n",
    "lda = LatentDirichletAllocation(n_topics = n_topics, random_state = 3, learning_method = 'online').fit_transform(X_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df['sentiment']\n",
    "Xlda_traincv, Xlda_test, ylda_traincv, ylda_test = train_test_split(lda,y, test_size = .2, random_state = 3)\n",
    "Xlda_train, Xlda_cv, ylda_train, ylda_cv = train_test_split(Xlda_traincv, ylda_traincv, test_size = .25, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_nb = MultinomialNB().fit(Xlda_train, ylda_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63244505520304772"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(lda_nb.predict(Xlda_test),ylda_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer()\n",
    "X_lda = tf_vectorizer.fit_transform(df['no_punc'])\n",
    "n_topics = 100\n",
    "lda = LatentDirichletAllocation(n_topics = n_topics, random_state = 3, learning_method = 'online').fit_transform(X_lda)\n",
    "Xlda_traincv, Xlda_test, ylda_traincv, ylda_test = train_test_split(lda,y, test_size = .2, random_state = 3)\n",
    "Xlda_train, Xlda_cv, ylda_train, ylda_cv = train_test_split(Xlda_traincv, ylda_traincv, test_size = .25, random_state = 3)\n",
    "lda_nb2 = MultinomialNB().fit(Xlda_train, ylda_train)\n",
    "accuracy_score(lda_nb2.predict(Xlda_test),ylda_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf_vectorizer = TfidfVectorizer(max_features = n_features)\n",
    "X_nmf = nmf_vectorizer.fit_transform(df['no_punc'])\n",
    "nmf = NMF(n_components = n_topics, random_state = 3).fit_transform(X_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df['sentiment']\n",
    "Xnmf_traincv, Xnmf_test, ynmf_traincv, ynmf_test = train_test_split(nmf,y, test_size = .2, random_state = 3)\n",
    "Xnmf_train, Xnmf_cv, ynmf_train, ynmf_cv = train_test_split(Xnmf_traincv, ynmf_traincv, test_size = .25, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6809429751108913"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_nb = MultinomialNB().fit(Xnmf_train,ynmf_train)\n",
    "accuracy_score(nmf_nb.predict(Xnmf_test),ynmf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf_vectorizer = TfidfVectorizer()\n",
    "X_nmf = nmf_vectorizer.fit_transform(df['no_punc'])\n",
    "nmf = NMF(n_components = n_topics, random_state = 3).fit_transform(X_nmf)\n",
    "Xnmf_traincv, Xnmf_test, ynmf_traincv, ynmf_test = train_test_split(nmf,y, test_size = .2, random_state = 3)\n",
    "Xnmf_train, Xnmf_cv, ynmf_train, ynmf_cv = train_test_split(Xnmf_traincv, ynmf_traincv, test_size = .25, random_state = 3)\n",
    "nmf_nb = MultinomialNB().fit(Xnmf_train,ynmf_train)\n",
    "accuracy_score(nmf_nb.predict(Xnmf_test),ynmf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No Embedding Layer, 25 node GloVe vectors, Vector Sum Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe Model\n",
      "Done. 1193514  words loaded!\n"
     ]
    }
   ],
   "source": [
    "vector_len = 25\n",
    "tokens = df['split']\n",
    "\n",
    "#Load the Glove model into Python\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading GloVe Model\")\n",
    "    with open(gloveFile,'r',encoding=\"utf8\") as f:\n",
    "        model = {}\n",
    "        for line in f:\n",
    "            splitLine = line.split()\n",
    "            word = splitLine[0]\n",
    "            embedding = [float(val) for val in splitLine[1:]]\n",
    "            model[word] = embedding\n",
    "        print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    " \n",
    "gloVe = loadGloveModel('glove.twitter.27B.25d.txt')\n",
    " \n",
    "def word_embed(tokens):\n",
    "    embeddings = []\n",
    "    for i in range(len(tokens)):\n",
    "        try:\n",
    "            embeddings.append(gloVe[tokens[i]])\n",
    "        except:\n",
    "            pass\n",
    "    return(embeddings)\n",
    "word_embeddings = [word_embed(t) for t in tokens]\n",
    " \n",
    " \n",
    "def doc_vectorizer(word_embeddings, vector_len):\n",
    "    sent_vec = np.zeros(vector_len)\n",
    "    for w in word_embeddings:\n",
    "        try:\n",
    "            sent_vec = np.add(sent_vec, w)\n",
    "        except:\n",
    "            pass\n",
    "    return sent_vec\n",
    "doc_embeddings = [doc_vectorizer(w,vector_len) for w in word_embeddings]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "X = np.array(doc_embeddings)\n",
    "y = to_categorical(np.asarray(df['sentiment'].values.tolist()))\n",
    "X_traincv, X_test, y_traincv, y_test = train_test_split(X,y, test_size = .2, random_state = 3)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_traincv, y_traincv, test_size = .25, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model_nn = Sequential()\n",
    "model_nn.add(Dense(25,activation = 'relu', input_shape = (X_train.shape[1],)))\n",
    "model_nn.add(Dense(2, activation = 'sigmoid'))\n",
    "early_stopping = EarlyStopping(patience = 2)\n",
    "model_nn.compile(optimizer = 'adam',loss = 'binary_crossentropy', metrics = ['accuracy'],callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "958368/958368 [==============================] - 39s - loss: 0.5917 - acc: 0.6835    - ETA: 1s - loss: 0.\n",
      "Epoch 2/10\n",
      "958368/958368 [==============================] - 37s - loss: 0.5762 - acc: 0.6942    - ETA:\n",
      "Epoch 3/10\n",
      "958368/958368 [==============================] - 33s - loss: 0.5727 - acc: 0.6970    \n",
      "Epoch 4/10\n",
      "958368/958368 [==============================] - 32s - loss: 0.5710 - acc: 0.6989    - ETA: 0s - loss: 0.5711 - acc\n",
      "Epoch 5/10\n",
      "958368/958368 [==============================] - 29s - loss: 0.5699 - acc: 0.6997    - E - ETA: 0s - loss: 0.5699 - acc: 0.69 - ETA: 0s - loss: 0.5699 - acc: 0.699\n",
      "Epoch 6/10\n",
      "958368/958368 [==============================] - 28s - loss: 0.5691 - acc: 0.7002    - ETA: 4s -  - ETA\n",
      "Epoch 7/10\n",
      "958368/958368 [==============================] - 30s - loss: 0.5686 - acc: 0.7011    - ETA: 9s -  - ETA: 4s - loss: 0.5685 - acc: 0.70 - ETA: - ETA: 0s - loss: 0.5686 - ac\n",
      "Epoch 8/10\n",
      "958368/958368 [==============================] - 28s - loss: 0.5681 - acc: 0.7012    - ETA: 9s - l - ETA: 6s - loss - ETA: 4s - loss: 0.567 - ETA: 4s - loss: 0.5678 - acc: - ETA: 3s - loss: 0 - ETA: 2s - loss: 0.5680 - - ETA: 2s - loss: 0.56 - ETA: 1s - loss: 0.5681 - acc: 0.701 - ETA: 1\n",
      "Epoch 9/10\n",
      "958368/958368 [==============================] - 29s - loss: 0.5679 - acc: 0.7015    - ETA: 8s - loss - ETA: 6s - loss: 0.5679 - ac - ETA: 6s - loss: - ETA: 5s - l - ETA: 4s -  - ETA: 0s - loss:\n",
      "Epoch 10/10\n",
      "958368/958368 [==============================] - 30s - loss: 0.5676 - acc: 0.7016    -  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xfb5f2f98>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319457/319457 [==============================] - 5s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.56880097532597584, 0.70029769264721076]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nn.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Layer, 25 node GloVe Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 516594 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['no_punc'].values.tolist())\n",
    "sequences = tokenizer.texts_to_sequences(df['no_punc'].values.tolist())\n",
    " \n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (1597281, 40)\n",
      "Shape of label tensor: (1597281, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    " \n",
    "data = pad_sequences(sequences)\n",
    "labels = to_categorical(np.asarray(df['sentiment'].values.tolist()))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "input_len = data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(0.2 * data.shape[0])\n",
    " \n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "vector_len = 25\n",
    "embeddings_index = {}\n",
    "with open('glove.twitter.27B.25d.txt', 'r',encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    " \n",
    "embedding_matrix = np.zeros((len(word_index) + 1, vector_len))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(516595, 25)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            vector_len,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=input_len,\n",
    "                            #batch_size=3,\n",
    "                            trainable=False)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 25)            12914875  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 25)                25025     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 2)                 52        \n",
      "=================================================================\n",
      "Total params: 12,939,952\n",
      "Trainable params: 25,077\n",
      "Non-trainable params: 12,914,875\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1277825 samples, validate on 319456 samples\n",
      "Epoch 1/10\n",
      "1277825/1277825 [==============================] - 64s - loss: 0.5683 - acc: 0.6988 - val_loss: 0.5552 - val_acc: 0.7095\n",
      "Epoch 2/10\n",
      "1277825/1277825 [==============================] - 64s - loss: 0.5504 - acc: 0.7143 - val_loss: 0.5491 - val_acc: 0.7152.71\n",
      "Epoch 3/10\n",
      "1277825/1277825 [==============================] - 64s - loss: 0.5442 - acc: 0.7190 - val_loss: 0.5456 - val_acc: 0.7182\n",
      "Epoch 4/10\n",
      "1277825/1277825 [==============================] - 63s - loss: 0.5404 - acc: 0.7222 - val_loss: 0.5428 - val_acc: 0.7206\n",
      "Epoch 5/10\n",
      "1277825/1277825 [==============================] - 58s - loss: 0.5379 - acc: 0.7242 - val_loss: 0.5423 - val_acc: 0.7208\n",
      "Epoch 6/10\n",
      "1277825/1277825 [==============================] - 62s - loss: 0.5361 - acc: 0.7258 - val_loss: 0.5425 - val_acc: 0.7215\n",
      "Epoch 7/10\n",
      "1277825/1277825 [==============================] - 65s - loss: 0.5346 - acc: 0.7270 - val_loss: 0.5415 - val_acc: 0.7226\n",
      "Epoch 8/10\n",
      "1277825/1277825 [==============================] - 57s - loss: 0.5334 - acc: 0.7279 - val_loss: 0.5419 - val_acc: 0.7226\n",
      "Epoch 9/10\n",
      "1277825/1277825 [==============================] - 64s - loss: 0.5326 - acc: 0.7285 - val_loss: 0.5435 - val_acc: 0.7210\n",
      "Epoch 10/10\n",
      "1277825/1277825 [==============================] - 65s - loss: 0.5318 - acc: 0.7292 - val_loss: 0.5421 - val_acc: 0.7217\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1926eb978>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "early_stopping = EarlyStopping(patience = 2)\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(25,activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=10,callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317888/319456 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.54206403729004027, 0.72171128418311126]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_val,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Layer, 50 node GloVe Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 516594 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['no_punc'].values.tolist())\n",
    "sequences = tokenizer.texts_to_sequences(df['no_punc'].values.tolist())\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences)\n",
    "labels = to_categorical(np.asarray(df['sentiment'].values.tolist()))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "input_len = data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(0.2 * data.shape[0])\n",
    " \n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "vector_len = 50\n",
    "embeddings_index = {}\n",
    "with open('glove.twitter.27B.50d.txt', 'r',encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    " \n",
    "embedding_matrix = np.zeros((len(word_index) + 1, vector_len))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 40, 50)            25829750  \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 25)                50025     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2)                 52        \n",
      "=================================================================\n",
      "Total params: 25,879,827\n",
      "Trainable params: 50,077\n",
      "Non-trainable params: 25,829,750\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1277825 samples, validate on 319456 samples\n",
      "Epoch 1/10\n",
      "1277825/1277825 [==============================] - 83s - loss: 0.5230 - acc: 0.7351 - val_loss: 0.5096 - val_acc: 0.7449\n",
      "Epoch 2/10\n",
      "1277825/1277825 [==============================] - 82s - loss: 0.5037 - acc: 0.7492 - val_loss: 0.5033 - val_acc: 0.7502\n",
      "Epoch 3/10\n",
      "1277825/1277825 [==============================] - 82s - loss: 0.4968 - acc: 0.7542 - val_loss: 0.5024 - val_acc: 0.7517\n",
      "Epoch 4/10\n",
      "1277825/1277825 [==============================] - 82s - loss: 0.4925 - acc: 0.7573 - val_loss: 0.5013 - val_acc: 0.7527\n",
      "Epoch 5/10\n",
      "1277825/1277825 [==============================] - 83s - loss: 0.4895 - acc: 0.7597 - val_loss: 0.4991 - val_acc: 0.7541\n",
      "Epoch 6/10\n",
      "1277825/1277825 [==============================] - 71s - loss: 0.4871 - acc: 0.7610 - val_loss: 0.5002 - val_acc: 0.7535\n",
      "Epoch 7/10\n",
      "1277825/1277825 [==============================] - 72s - loss: 0.4853 - acc: 0.7623 - val_loss: 0.5018 - val_acc: 0.7529\n",
      "Epoch 8/10\n",
      "1277825/1277825 [==============================] - 72s - loss: 0.4837 - acc: 0.7637 - val_loss: 0.5000 - val_acc: 0.7537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17ef78438>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            vector_len,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=input_len,\n",
    "                            #batch_size=3,\n",
    "                            trainable=False)\n",
    "early_stopping = EarlyStopping(patience = 2)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(25,activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=10,callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Layer, 100 node GloVe Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "vector_len = 100\n",
    "embeddings_index = {}\n",
    "with open('glove.twitter.27B.100d.txt', 'r',encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    " \n",
    "embedding_matrix = np.zeros((len(word_index) + 1, vector_len))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 40, 100)           51659500  \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 25)                100025    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 2)                 52        \n",
      "=================================================================\n",
      "Total params: 51,759,577\n",
      "Trainable params: 100,077\n",
      "Non-trainable params: 51,659,500\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1277825 samples, validate on 319456 samples\n",
      "Epoch 1/10\n",
      "1277825/1277825 [==============================] - 103s - loss: 0.4941 - acc: 0.7571 - val_loss: 0.4794 - val_acc: 0.7670\n",
      "Epoch 2/10\n",
      "1277825/1277825 [==============================] - 102s - loss: 0.4712 - acc: 0.7731 - val_loss: 0.4745 - val_acc: 0.7701\n",
      "Epoch 3/10\n",
      "1277825/1277825 [==============================] - 101s - loss: 0.4619 - acc: 0.7789 - val_loss: 0.4716 - val_acc: 0.7739\n",
      "Epoch 4/10\n",
      "1277825/1277825 [==============================] - 102s - loss: 0.4557 - acc: 0.7827 - val_loss: 0.4726 - val_acc: 0.7731\n",
      "Epoch 5/10\n",
      "1277825/1277825 [==============================] - 102s - loss: 0.4514 - acc: 0.7855 - val_loss: 0.4701 - val_acc: 0.7747\n",
      "Epoch 6/10\n",
      "1277825/1277825 [==============================] - 102s - loss: 0.4477 - acc: 0.7878 - val_loss: 0.4722 - val_acc: 0.7740\n",
      "Epoch 7/10\n",
      "1277825/1277825 [==============================] - 102s - loss: 0.4447 - acc: 0.7893 - val_loss: 0.4738 - val_acc: 0.7739\n",
      "Epoch 8/10\n",
      "1277825/1277825 [==============================] - 102s - loss: 0.4422 - acc: 0.7910 - val_loss: 0.4753 - val_acc: 0.7734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17e618940>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            vector_len,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=input_len,\n",
    "                            #batch_size=3,\n",
    "                            trainable=False)\n",
    "early_stopping = EarlyStopping(patience = 2)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(25,activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=10,callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Layer, 200 node GloVe Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "vector_len = 200\n",
    "embeddings_index = {}\n",
    "with open('glove.twitter.27B.200d.txt', 'r',encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    " \n",
    "embedding_matrix = np.zeros((len(word_index) + 1, vector_len))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 40, 200)           103319000 \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 25)                200025    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 2)                 52        \n",
      "=================================================================\n",
      "Total params: 103,519,077\n",
      "Trainable params: 200,077\n",
      "Non-trainable params: 103,319,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1277825 samples, validate on 319456 samples\n",
      "Epoch 1/10\n",
      "1277825/1277825 [==============================] - 154s - loss: 0.4757 - acc: 0.7695 - val_loss: 0.4618 - val_acc: 0.7788\n",
      "Epoch 2/10\n",
      "1277825/1277825 [==============================] - 154s - loss: 0.4516 - acc: 0.7853 - val_loss: 0.4569 - val_acc: 0.7820\n",
      "Epoch 3/10\n",
      "1277825/1277825 [==============================] - 154s - loss: 0.4404 - acc: 0.7923 - val_loss: 0.4555 - val_acc: 0.7828\n",
      "Epoch 4/10\n",
      "1277825/1277825 [==============================] - 154s - loss: 0.4322 - acc: 0.7974 - val_loss: 0.4610 - val_acc: 0.7825\n",
      "Epoch 5/10\n",
      "1277825/1277825 [==============================] - 154s - loss: 0.4258 - acc: 0.8009 - val_loss: 0.4583 - val_acc: 0.7836\n",
      "Epoch 6/10\n",
      "1277825/1277825 [==============================] - 154s - loss: 0.4203 - acc: 0.8040 - val_loss: 0.4621 - val_acc: 0.7830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4f86978>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            vector_len,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=input_len,\n",
    "                            #batch_size=3,\n",
    "                            trainable=False)\n",
    "early_stopping = EarlyStopping(patience = 2)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(25,activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=10,callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25 node GloVe layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "vector_len = 25\n",
    "embeddings_index = {}\n",
    "with open('glove.twitter.27B.25d.txt', 'r',encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    " \n",
    "embedding_matrix = np.zeros((len(word_index) + 1, vector_len))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 40, 25)            12914875  \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 100)               50400     \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 25)                2525      \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 2)                 52        \n",
      "=================================================================\n",
      "Total params: 12,967,852\n",
      "Trainable params: 52,977\n",
      "Non-trainable params: 12,914,875\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1277825 samples, validate on 319456 samples\n",
      "Epoch 1/10\n",
      "1277825/1277825 [==============================] - 1080s - loss: 0.4988 - acc: 0.7527 - val_loss: 0.4605 - val_acc: 0.7804\n",
      "Epoch 2/10\n",
      "1277825/1277825 [==============================] - 1067s - loss: 0.4449 - acc: 0.7898 - val_loss: 0.4347 - val_acc: 0.7958\n",
      "Epoch 3/10\n",
      "1277825/1277825 [==============================] - 1060s - loss: 0.4260 - acc: 0.8012 - val_loss: 0.4219 - val_acc: 0.8035\n",
      "Epoch 4/10\n",
      "1277825/1277825 [==============================] - 1059s - loss: 0.4148 - acc: 0.8078 - val_loss: 0.4195 - val_acc: 0.8039\n",
      "Epoch 5/10\n",
      "1277825/1277825 [==============================] - 1069s - loss: 0.4064 - acc: 0.8129 - val_loss: 0.4151 - val_acc: 0.8087\n",
      "Epoch 6/10\n",
      "1277825/1277825 [==============================] - 1060s - loss: 0.4001 - acc: 0.8169 - val_loss: 0.4052 - val_acc: 0.8138\n",
      "Epoch 7/10\n",
      "1277825/1277825 [==============================] - 1059s - loss: 0.3949 - acc: 0.8196 - val_loss: 0.4039 - val_acc: 0.8141\n",
      "Epoch 8/10\n",
      "1277825/1277825 [==============================] - 1059s - loss: 0.3906 - acc: 0.8221 - val_loss: 0.4051 - val_acc: 0.8146\n",
      "Epoch 9/10\n",
      "1277825/1277825 [==============================] - 1062s - loss: 0.3869 - acc: 0.8238 - val_loss: 0.4051 - val_acc: 0.8137\n",
      "Epoch 10/10\n",
      "1277825/1277825 [==============================] - 1060s - loss: 0.3838 - acc: 0.8258 - val_loss: 0.4024 - val_acc: 0.8165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x221f32978>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "early_stopping = EarlyStopping(patience = 2)\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            vector_len,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=input_len,\n",
    "                            #batch_size=3,\n",
    "                            trainable=False)\n",
    "\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(embedding_layer)\n",
    "model_rnn.add(LSTM(100))\n",
    "model_rnn.add(Dense(25,activation='relu'))\n",
    "model_rnn.add(Dense(2, activation='sigmoid'))\n",
    "model_rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_rnn.summary())\n",
    "model_rnn.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=10, batch_size = 256, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50 node GLoVe layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "vector_len = 50\n",
    "embeddings_index = {}\n",
    "with open('glove.twitter.27B.50d.txt', 'r',encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    " \n",
    "embedding_matrix = np.zeros((len(word_index) + 1, vector_len))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 40, 50)            25829750  \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 25)                2525      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 2)                 52        \n",
      "=================================================================\n",
      "Total params: 25,892,727\n",
      "Trainable params: 62,977\n",
      "Non-trainable params: 25,829,750\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1277825 samples, validate on 319456 samples\n",
      "Epoch 1/10\n",
      "1277825/1277825 [==============================] - 979s - loss: 0.4549 - acc: 0.7831 - val_loss: 0.4216 - val_acc: 0.8050\n",
      "Epoch 2/10\n",
      "1277825/1277825 [==============================] - 909s - loss: 0.4126 - acc: 0.8098 - val_loss: 0.4043 - val_acc: 0.8151\n",
      "Epoch 3/10\n",
      "1277825/1277825 [==============================] - 866s - loss: 0.3975 - acc: 0.8185 - val_loss: 0.4030 - val_acc: 0.8154\n",
      "Epoch 4/10\n",
      "1277825/1277825 [==============================] - 866s - loss: 0.3879 - acc: 0.8236 - val_loss: 0.3919 - val_acc: 0.8214\n",
      "Epoch 5/10\n",
      "1277825/1277825 [==============================] - 866s - loss: 0.3811 - acc: 0.8277 - val_loss: 0.3906 - val_acc: 0.8227\n",
      "Epoch 6/10\n",
      "1277825/1277825 [==============================] - 866s - loss: 0.3755 - acc: 0.8309 - val_loss: 0.3876 - val_acc: 0.8242\n",
      "Epoch 7/10\n",
      "1277825/1277825 [==============================] - 865s - loss: 0.3710 - acc: 0.8334 - val_loss: 0.3914 - val_acc: 0.8213\n",
      "Epoch 8/10\n",
      "1277825/1277825 [==============================] - 866s - loss: 0.3670 - acc: 0.8358 - val_loss: 0.3870 - val_acc: 0.8242\n",
      "Epoch 9/10\n",
      "1277825/1277825 [==============================] - 866s - loss: 0.3636 - acc: 0.8372 - val_loss: 0.3862 - val_acc: 0.8255\n",
      "Epoch 10/10\n",
      "1277825/1277825 [==============================] - 867s - loss: 0.3608 - acc: 0.8390 - val_loss: 0.3898 - val_acc: 0.8237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17e151c18>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "early_stopping = EarlyStopping(patience = 2)\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            vector_len,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=input_len,\n",
    "                            #batch_size=3,\n",
    "                            trainable=False)\n",
    "\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(embedding_layer)\n",
    "model_rnn.add(LSTM(100))\n",
    "model_rnn.add(Dense(25,activation='relu'))\n",
    "model_rnn.add(Dense(2, activation='sigmoid'))\n",
    "model_rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_rnn.summary())\n",
    "model_rnn.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=10, batch_size = 256, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100 node GLoVe layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "vector_len = 100\n",
    "embeddings_index = {}\n",
    "with open('glove.twitter.27B.100d.txt', 'r',encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    " \n",
    "embedding_matrix = np.zeros((len(word_index) + 1, vector_len))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 40, 100)           51659500  \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 25)                2525      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 2)                 52        \n",
      "=================================================================\n",
      "Total params: 51,742,477\n",
      "Trainable params: 82,977\n",
      "Non-trainable params: 51,659,500\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1277825 samples, validate on 319456 samples\n",
      "Epoch 1/10\n",
      "1277825/1277825 [==============================] - 939s - loss: 0.4319 - acc: 0.7981 - val_loss: 0.4038 - val_acc: 0.8155\n",
      "Epoch 2/10\n",
      "1277825/1277825 [==============================] - 937s - loss: 0.3944 - acc: 0.8206 - val_loss: 0.3906 - val_acc: 0.8228\n",
      "Epoch 3/10\n",
      "1277825/1277825 [==============================] - 937s - loss: 0.3802 - acc: 0.8284 - val_loss: 0.3841 - val_acc: 0.8262\n",
      "Epoch 4/10\n",
      "1277825/1277825 [==============================] - 942s - loss: 0.3706 - acc: 0.8335 - val_loss: 0.3787 - val_acc: 0.8298\n",
      "Epoch 5/10\n",
      "1277825/1277825 [==============================] - 937s - loss: 0.3631 - acc: 0.8376 - val_loss: 0.3777 - val_acc: 0.8294\n",
      "Epoch 6/10\n",
      "1277825/1277825 [==============================] - 947s - loss: 0.3576 - acc: 0.8406 - val_loss: 0.3778 - val_acc: 0.8301\n",
      "Epoch 7/10\n",
      "1277825/1277825 [==============================] - 942s - loss: 0.3525 - acc: 0.8432 - val_loss: 0.3767 - val_acc: 0.8303\n",
      "Epoch 8/10\n",
      "1277825/1277825 [==============================] - 939s - loss: 0.3484 - acc: 0.8451 - val_loss: 0.3785 - val_acc: 0.8298\n",
      "Epoch 9/10\n",
      "1277825/1277825 [==============================] - 937s - loss: 0.3446 - acc: 0.8474 - val_loss: 0.3791 - val_acc: 0.8298\n",
      "Epoch 10/10\n",
      "1277825/1277825 [==============================] - 937s - loss: 0.3411 - acc: 0.8489 - val_loss: 0.3817 - val_acc: 0.8294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c193ac88>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "early_stopping = EarlyStopping(patience = 2)\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            vector_len,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=input_len,\n",
    "                            #batch_size=3,\n",
    "                            trainable=False)\n",
    "\n",
    "model_rnn_100 = Sequential()\n",
    "model_rnn_100.add(embedding_layer)\n",
    "model_rnn_100.add(LSTM(100))\n",
    "model_rnn_100.add(Dense(25,activation='relu'))\n",
    "model_rnn_100.add(Dense(2, activation='sigmoid'))\n",
    "model_rnn_100.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_rnn_100.summary())\n",
    "model_rnn_100.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=10, batch_size = 256, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 200 node GLoVe layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "vector_len = 200\n",
    "embeddings_index = {}\n",
    "with open('glove.twitter.27B.200d.txt', 'r',encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    " \n",
    "embedding_matrix = np.zeros((len(word_index) + 1, vector_len))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 40, 50)            25829750  \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 25)                2525      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 2)                 52        \n",
      "=================================================================\n",
      "Total params: 25,892,727\n",
      "Trainable params: 62,977\n",
      "Non-trainable params: 25,829,750\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1277825 samples, validate on 319456 samples\n",
      "Epoch 1/10\n",
      "1277825/1277825 [==============================] - 1157s - loss: 0.4182 - acc: 0.8068 - val_loss: 0.3939 - val_acc: 0.8213\n",
      "Epoch 2/10\n",
      "1277825/1277825 [==============================] - 1157s - loss: 0.3813 - acc: 0.8278 - val_loss: 0.3790 - val_acc: 0.8293\n",
      "Epoch 3/10\n",
      "1277825/1277825 [==============================] - 1168s - loss: 0.3666 - acc: 0.8358 - val_loss: 0.3768 - val_acc: 0.8303\n",
      "Epoch 4/10\n",
      "1277825/1277825 [==============================] - 1168s - loss: 0.3564 - acc: 0.8412 - val_loss: 0.3722 - val_acc: 0.8330\n",
      "Epoch 5/10\n",
      "1277825/1277825 [==============================] - 1294s - loss: 0.3483 - acc: 0.8455 - val_loss: 0.3712 - val_acc: 0.8339\n",
      "Epoch 6/10\n",
      "1277825/1277825 [==============================] - 1339s - loss: 0.3413 - acc: 0.8490 - val_loss: 0.3731 - val_acc: 0.8343\n",
      "Epoch 7/10\n",
      "1277825/1277825 [==============================] - 1349s - loss: 0.3354 - acc: 0.8522 - val_loss: 0.3746 - val_acc: 0.8332\n",
      "Epoch 8/10\n",
      "1277825/1277825 [==============================] - 1414s - loss: 0.3299 - acc: 0.8547 - val_loss: 0.3766 - val_acc: 0.8322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18e174c50>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "early_stopping = EarlyStopping(patience = 2)\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            vector_len,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=input_len,\n",
    "                            #batch_size=3,\n",
    "                            trainable=False)\n",
    "\n",
    "model_rnn_200 = Sequential()\n",
    "model_rnn_200.add(embedding_layer)\n",
    "model_rnn_200.add(LSTM(100))\n",
    "model_rnn_200.add(Dense(25,activation='relu'))\n",
    "model_rnn_200.add(Dense(2, activation='sigmoid'))\n",
    "model_rnn_200.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_rnn.summary())\n",
    "model_rnn_200.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=10, batch_size = 256, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 200 node GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 40, 50)            25829750  \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 25)                2525      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 2)                 52        \n",
      "=================================================================\n",
      "Total params: 25,892,727\n",
      "Trainable params: 62,977\n",
      "Non-trainable params: 25,829,750\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1277825 samples, validate on 319456 samples\n",
      "Epoch 1/10\n",
      "1277825/1277825 [==============================] - 779s - loss: 0.4663 - acc: 0.7770 - val_loss: 0.4488 - val_acc: 0.7876\n",
      "Epoch 2/10\n",
      "1277825/1277825 [==============================] - 921s - loss: 0.4388 - acc: 0.7938 - val_loss: 0.4414 - val_acc: 0.7925\n",
      "Epoch 3/10\n",
      "1277825/1277825 [==============================] - 732s - loss: 0.4258 - acc: 0.8016 - val_loss: 0.4401 - val_acc: 0.7929\n",
      "Epoch 4/10\n",
      "1277825/1277825 [==============================] - 782s - loss: 0.4159 - acc: 0.8075 - val_loss: 0.4407 - val_acc: 0.7933\n",
      "Epoch 5/10\n",
      "1277825/1277825 [==============================] - 803s - loss: 0.4075 - acc: 0.8124 - val_loss: 0.4441 - val_acc: 0.7930\n",
      "Epoch 6/10\n",
      "1277825/1277825 [==============================] - 806s - loss: 0.4006 - acc: 0.8162 - val_loss: 0.4433 - val_acc: 0.7922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18e633908>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "\n",
    "early_stopping = EarlyStopping(patience = 2)\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            vector_len,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=input_len,\n",
    "                            #batch_size=3,\n",
    "                            trainable=False)\n",
    "model_cnn_200 = Sequential()\n",
    "model_cnn_200.add(embedding_layer)\n",
    "model_cnn_200.add(Conv1D(128,5,activation = 'relu'))\n",
    "model_cnn_200.add(MaxPooling1D(5))\n",
    "model_cnn_200.add(Flatten())\n",
    "model_cnn_200.add(Dense(25, activation = 'relu'))\n",
    "model_cnn_200.add(Dense(2, activation = 'sigmoid'))\n",
    "model_cnn_200.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_rnn.summary())\n",
    "model_cnn_200.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=10, callbacks = [early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    " \n",
    "sequence_input = Input(shape=(368,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(9)(x) # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(2, activation='softmax')(x)\n",
    " \n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    " \n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=50, batch_size=128)\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
